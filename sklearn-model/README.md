# Pipeline MLOps Inteligente

Sistema integrado de Machine Learning e Inteligencia Artificial con tres componentes principales:
- **LLM Service**: ConversaciÃ³n con Google Gemini 2.5 Flash
- **ML Service**: PredicciÃ³n de supervivencia Titanic (RandomForest)
- **CNN Service**: ClasificaciÃ³n de imÃ¡genes CIFAR-10 (5 clases)

## Arquitectura

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Gradio Frontend â”‚
â”‚   (Port 7860)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         â”‚         â”‚          â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ LLM  â”‚  â”‚  ML  â”‚  â”‚  CNN  â”‚  â”‚ MLflow  â”‚
â”‚ :8000â”‚  â”‚ :8001â”‚  â”‚ :8002 â”‚  â”‚  :5000  â”‚
â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                                     â”‚
                                â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                                â”‚  MinIO  â”‚
                                â”‚  :9000  â”‚
                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

## TecnologÃ­as

- **Backend**: Python 3.10, FastAPI, Uvicorn
- **ML/DL**: Scikit-learn, TensorFlow/Keras
- **LLM**: Google Gemini 2.5 Flash
- **Frontend**: Gradio
- **MLOps**: MLflow, MinIO (S3)
- **OrquestaciÃ³n**: Docker Swarm
- **CI/CD**: GitHub Actions

## Quick Start

\`\`\`bash
# 1. Clonar repositorio
git clone https://github.com/tu-usuario/mlops-pipeline-inteligente.git
cd mlops-pipeline-inteligente

# 2. Configurar variables de entorno
cp infra/.env.example infra/.env
# Editar .env con tu GOOGLE_API_KEY

# 3. Inicializar Swarm
docker swarm init

# 4. Build de imÃ¡genes
cd infra
.\build-local.ps1

# 5. Deploy del stack
.\deploy-stack.ps1

# 6. Acceder a servicios
# Gradio UI:    http://localhost:7860
# MLflow UI:    http://localhost:5000
# MinIO Console: http://localhost:9001
\`\`\`

## Tests

\`\`\`bash
# Ejecutar todos los tests
.\run-tests.ps1

# Tests individuales
cd llm-connector && pytest tests/ -v
cd sklearn-model && pytest tests/ -v
cd cnn-image && pytest tests/ -v
cd gradio-frontend && pytest tests/ -v
\`\`\`

## Estructura del Proyecto

\`\`\`
mlops-pipeline-inteligente/
â”œâ”€â”€ llm-connector/          # Servicio LLM (Gemini)
â”œâ”€â”€ sklearn-model/          # Servicio ML (Titanic)
â”œâ”€â”€ cnn-image/              # Servicio CNN (CIFAR-10)
â”œâ”€â”€ gradio-frontend/        # Interfaz web
â”œâ”€â”€ mlflow-server/          # Servidor MLflow
â”œâ”€â”€ infra/                  # Docker Compose/Swarm
â””â”€â”€ .github/workflows/      # CI/CD
\`\`\`

## Autor

Christian Gomez - Proyecto Final MLOps
"@ | Out-File -FilePath README.md -Encoding utf8

# Crear .env.example
Write-Host "ğŸ“ Creando .env.example..." -ForegroundColor Yellow
@"
# Variables de entorno para MLOps Pipeline

# Google Gemini API
GOOGLE_API_KEY=your_api_key_here
GOOGLE_MODEL=gemini-2.5-flash

# MLflow
MLFLOW_TRACKING_URI=http://mlflow-server:5000
MLFLOW_S3_ENDPOINT_URL=http://minio:9000

# MinIO
AWS_ACCESS_KEY_ID=minioadmin
AWS_SECRET_ACCESS_KEY=minioadmin
AWS_DEFAULT_REGION=us-east-1
"@ | Out-File -FilePath infra/.env.example -Encoding utf8

# Commit inicial
Write-Host "`nâœ… Commit 1: Initial commit" -ForegroundColor Green
git add .gitignore README.md infra/.env.example
git commit -m "chore: initial commit with project structure"

# Crear branch develop
git checkout -b develop